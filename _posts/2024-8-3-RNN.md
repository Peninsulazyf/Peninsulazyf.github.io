---
layout: post
title: "RNN详解"
date:   2024-8-3
comments: true
author: YufengZhang
---

###### 说明：对于想要入门大模型的小白，RNN作为NLP的基础一定得要好好学啦，让我们一起学习RNN叭！

<!-- more -->

### 目录
- [一、为什么需要RNN](#一、为什么需要RNN)
- [二、RNN的架构](#二、RNN的架构)
- [三、通过时间反向传播](#三、通过时间反向传播)
- [四、RNN的多种架构](#四、RNN的多种架构)
## 一、为什么需要RNN

首先，让我们了解一下**序列**是什么。序列在深度学习中一般指的是带有**时间先后顺序**（拥有逻辑结构）的一段具有**连续关系**的数据（例如文本，语音等）。下图展示了一些常见的序列信息：

![2](https://peninsulazyf.github.io/images/2.png)

由此可见，序列可以应用在众多领域，包括但不限于：语音识别、音乐发生器、情感分析、DNA序列分析、机器翻译、视频动作识别和命名实体识别...

在NLP任务中，一般都是**Seq2Seq**（Sequence to Sequence）的任务。但是，传统神经网络每一层是相互独立的，同一层中每个节点之间也是相互独立的，因此传统神经网络**无法共享**输入序列之间的特征。并且，不同样本的**输入序列长度和输出序列长度不同**，造成传统神经网络很难做到全部统一。解决办法之一是设定一个最大序列长度，对每个输入和输出序列补零并统一到最大长度。但是这种做法实际效果并不理想。

![1](https://peninsulazyf.github.io/images/1.png)

但是，RNN可以很好解决上述问题。RNN很擅长根据上文信息，对接下来的词进行推理（就比如“我要吃...”，按照语法规则，后面接上名词的概率比较大。如果后面再出现动词的话，就大概率不符合语言逻辑的），因此能够很好地**共享输入序列**之间的特征。并且，RNN的**架构也十分灵活**（这一部分我会在后面详细介绍哈~），所以处理任意长度的输入序列和输出序列都是十分轻松的！

## 二、RNN的架构

那么RNN的架构长什么样呢？下图展示了RNN的一般架构：

![3](https://peninsulazyf.github.io/images/3.png)

其中 $a^{<0>}$ 一般都是零向量。那么以 $t=1$ 为例，具体的计算流程如下：

$$a^{<1>}=g_1(W_{aa} \cdot a^{<0>}+W_{ax} \cdot x^{<1>}+b_a)$$

其中，激活函数一般会选择使用 $tanh$ 或 $ReLu$ 。

$$\hat{y}^{<1>}=g_2(W_{ya} \cdot a^{<1>}+b_y)$$

其中，激活函数一般会选择使用 $Sigmoid$ 。

那么推广到 $t$ ，公式就是这样啦：

$$a^{<t>}=g(W_{aa} \cdot a^{<t-1>}+W_{ax} \cdot x^{<t>}+b_a)$$

$$\hat{y}^{<t>}=g(W_{ya} \cdot a^{<t>}+b_y)$$

那么为了简化表达式，可以对 $a^{t}$ 项进行整合：将 $W_{aa}$ 和 $W_{ax}$ 水平排列为一个矩阵 $W_a$ ，同时将 $a^{t-1}$ 和 $x^{t}$ 堆叠成一个矩阵，则有：

(Bug：这里的行内公式的 $t$ 或者 $t-1$ 都需要带上**尖括号** “<>”，这里的尖括号代表的是**时间步** $t$ ，但是由于有Bug，就没有带了哈...)

![4](https://peninsulazyf.github.io/images/4.png)

大致的RNN架构就像上面所述，值得一提的是，上面的RNN准确说是单向的RNN（left-to-right），因此 时间步 $t$ 时刻的 $\hat{y}$ 只与左边 $t-1$ 的元素有关，但是有时候也与右边的元素有关，因此后面会讲到双向RNN，简称为BRNN。

## 三、通过时间反向传播

为了计算反向传播过程，在**单个时间步**上某个单词预测值的损失函数采用**交叉熵损失函数**，单个元素的 $Loss function$ 如下所示：

![5](https://peninsulazyf.github.io/images/5.png)

将单个位置上的损失函数相加，得到该样本**所有元素**的 $Loss function$ 为：

![6](https://peninsulazyf.github.io/images/6.png)

RNN的反向传播又被称为**通过时间反向传播（Backpropagation through time）**，从右向左的计算就像是时间倒流，其过程就是在求偏导数，思路与传统神经网络一致。下图展示了更加详细的计算过程：

![7](https://peninsulazyf.github.io/images/7.png)

## 四、RNN的多种架构

由于输入序列和输出序列的长度并不一致。因此，根据所需的输入序列和输出序列长度，可以将网络分为：

![8](https://peninsulazyf.github.io/images/8.png)

不同类别的结构示意图如下：

![9](https://peninsulazyf.github.io/images/9.png)

其中，第二种Many to Many的架构值得关注下，因为这便是Encoder-Decoder架构的雏形。

