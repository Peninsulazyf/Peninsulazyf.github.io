---
layout: post
title: "RNN详解"
date:   2024-8-3
comments: true
author: YufengZhang
---

###### 说明：对于想要入门大模型的小白，RNN作为NLP的基础一定得要好好学啦，让我们一起学习RNN叭！

<!-- more -->

### 目录
- [一、为什么需要RNN](#一、为什么需要RNN)
- 
## 一、为什么需要RNN

首先，让我们了解一下**序列**是什么。序列在深度学习中一般指的是带有**时间先后顺序**（拥有逻辑结构）的一段具有**连续关系**的数据（例如文本，语音等）。下图展示了一些常见的序列信息：

![2](https://peninsulazyf.github.io/images/2.png)

由此可见，序列可以应用在众多领域，包括但不限于：语音识别、音乐发生器、情感分析、DNA序列分析、机器翻译、视频动作识别和命名实体识别...

在NLP任务中，一般都是**Seq2Seq**（Sequence to Sequence）的任务。但是，传统神经网络每一层是相互独立的，同一层中每个节点之间也是相互独立的，因此传统神经网络**无法共享**输入序列之间的特征。并且，不同样本的**输入序列长度和输出序列长度不同**，造成传统神经网络很难做到全部统一。解决办法之一是设定一个最大序列长度，对每个输入和输出序列补零并统一到最大长度。但是这种做法实际效果并不理想。

![1](https://peninsulazyf.github.io/images/1.png)

但是，RNN可以很好解决上述问题。RNN很擅长根据上文信息，对接下来的词进行推理（就比如“我要吃...”，按照语法规则，后面接上名词的概率比较大。如果后面再出现动词的话，就大概率不符合语言逻辑的），因此能够很好地**共享输入序列**之间的特征。并且，RNN的**架构也十分灵活**（这一部分我会在后面详细介绍哈~），所以处理任意长度的输入序列和输出序列都是十分轻松的！

## 二、RNN的架构

那么RNN的架构长什么样呢？下图展示了RNN的一般架构：

![3](https://peninsulazyf.github.io/images/3.png)

其中 \($a^{<0>}$\) 一般都是零向量。那么以 $t=1$ 为例，具体的计算流程如下：

$$a^{<1>}=g_1(W_{aa} \cdot a^{<0>}+W_{ax} \cdot x^{<1>}+b_a)$$

其中，激活函数一般会选择使用 $tanh$ 或 $ReLu$ 。

$$\hat{y}^{<1>}=g_2(W_{ya} \cdot a^{<1>}+b_y)$$

其中，激活函数一般会选择使用 $Sigmoid$ 。

那么推广到 $t$ ，公式就是这样啦：

$$a^{<t>}=g(W_{aa} \cdot a^{<t-1>}+W_{ax} \cdot x^{<t>}+b_a)$$

$$\hat{y}^{<t>}=g(W_{ya} \cdot a^{<t>}+b_y)$$

那么为了简化表达式，可以对于 $a^{<t>}$ 项进行整合：将 $W_{aa}$ 和 $W_{ax}$ 水平排列为一个矩阵 $W_a$ ，同时将 $a^{<t-1>}$ 和 $ x^{<t>}$ 堆叠成一个矩阵，则有：

![4](https://peninsulazyf.github.io/images/4.png)

因此，可以
